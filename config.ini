[dirs]
# Stores summaries and final model checkpoints (should be backed up).
log = ./log

# Path to the root directory of training data
data = ../KITTI_raw/raw_data_downloader

# Path to the root directory of validation dataset
val_data_path = ../KITTI_Optical_flow/data_stereo_flow/training


[run]
# Number of threads for loading input examples
num_input_threads = 1

# Total batch.
batch_size = 4

# Dataset to *train* on.
# One of {kitti, kitti_2012_2015_multiview}.
dataset = kitti

# Dataset for validation during training
# One of {kitti_2012_train, kitti_2015_train}.
val_dataset = kitti_2012_train


[train]

# Whetrher the training data should separately contain the reverse sequences of the image sequences
swap_sequences = False

# Interval for halving the learning rate
decay_interval = 100000

# Interval for saving checkpoints.
# After each save, training is interrupted, the checkpoint is evaluated,
# and training is continued from that checkpoint.
save_interval = 2000

# Interval for displaying and saving training logs
display_interval = 500

# Specify the network architecture to train on
# One of {flownet, pwcnet}
network = flownet

# Specify architecture type for flownet using the naming convention from the paper
# (borrowed from FlowNet 2.0, https://arxiv.org/abs/1612.01925).
# E.g. C to train UnFlow-C, CS to train UnFlow-CSS, CSS to train UnFlow-CSS.
# Use small letters to use smaller networks, as in FlowNet 2.0.
# (This option is not considered for pwcnet)
flownet = C

# If unspecified, only the final network is trained and any previous networks are kept fixed.
# Currently, end-to-end training is only supported for SUPERVISED training,
# i.e., uncomment this when run['dataset'] = kitti_ft.
#train_all = true

# Names of experiments to use for initializing the network(s).
# Comma separated, one name for each network to initialize from a different experiment.
# E.g., when training UnFlowCSS, use UnFlowC,UnFlowCS to use the UnFlowC experiment for
# first network and UnFlowCS for the second network.
#finetune = my_experiment_pwc


# Set it to true if the losses are to be computed upto full resolution.
# If false, losses are computed only till 1/4 th of the original resolution.
# DO NOT UNCOMMENT (default set it as False)
#full_res = False

# Channel means for inputs
normalization = 104.920005,110.1753,114.785955

# Compute a loss at each stage when training in an unsupervised way,
# i.e. when dataset is not kitti_ft.
# JUST LEAVE IT SET TO TRUE ALWAYS TO REPLICATE THE RESULTS
pyramid_loss = True

# -----------------------------------------------------------------------------
# Masking & occlusion handling

# Occlusion detectioN. Use 'fb' for occlusion estimation by forward backward consistency or
# commnet it to avoid occlusion handling during training.
mask_occlusion = fb

# Mask border regions in data term
# JUST LEAVE THIS SET TO TRUE
border_mask = True

# -----------------------------------------------------------------------------
# Data term (multiple terms may be combined)

# Encourages forward and backward flow to be opposite to each other (if not masked)
#fb_weight = 0.2

# Gradient error between backward warped second image and first image.
# NOT TESTED YET - USE ON YOUR OWN RISK
grad_weight = 1.0

# Color error between backward warped second image and first image.
#photo_weight = 1.0

# Ternary transform error between backward warped second image and first image.
ternary_weight = 1.0

# -----------------------------------------------------------------------------
# Regularization (ONLY ONE LINE OF THESE SHOULD BE UNCOMMENTED)

# Use first order smoothness
#smooth_1st_weight = 3.0

# Use second order smoothness
smooth_2nd_weight = 3.0
#smooth_2nd_edge_weight = 3.0

# Resolution of Images for validation
val_height = 384
val_width = 1280

# -----------------------------------------------------------------------------
# SETTINGS IN THE train_{} CATEGORIES (BELOW, e.g. train_kitti) OVERWRITE GLOBAL
# TRAINING SETTINGS. One category for each training dataset.
# -----------------------------------------------------------------------------
# For each dataset, height, width, num_iters, learning_rate and decay_after are
# required. All other parameters are read from the global section if not specified.


[train_kitti_ft]
height = 320
width = 768

# Fine grained learning schedule for fine-tuning.
# Both lists must have the same number of comma separated elements
manual_decay_iters = 45000,20000,20000,10000,2500,2500
manual_decay_lrs = 0.5e-5,0.25e-5,0.1e-5,0.05e-5,0.25e-6,0.1e-6


[train_kitti]
height = 320
width = 1152
num_iters = 500000
learning_rate = 1.0e-4
decay_after = 100000
fb_weight = 0.2
mask_occlusion = fb
occ_weight = 12.4

[train_kitti_2012_2015_multiview]
height = 320
width = 1152
num_iters = 500000
learning_rate = 1.0e-4
decay_after = 100000
fb_weight = 0.2
mask_occlusion = fb
occ_weight = 12.4

